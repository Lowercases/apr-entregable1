{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "APR_E1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhkpffGffGov"
      },
      "source": [
        "# APR 2021 Entregable 1: MDPs y Programación Dinámica\n",
        "\n",
        "## Total: 100 puntos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrigVk6uzMh9"
      },
      "source": [
        "Temas tratados en este entregable:\n",
        "\n",
        "\n",
        "\n",
        "*   Markov Decission Processes (MDPs)\n",
        "    * Cálculo de probabilidades de transición en T pasos\n",
        "    * Ejemplo: \"Student MDP\"\n",
        "\n",
        "*   Programación Dinámica:\n",
        "    * Policy Improvement Theorem\n",
        "    * Policy Evaluation\n",
        "    * Policy Iteration\n",
        "    * Value Iteration\n",
        "\n",
        "\n",
        "\n",
        "Se pide responder algunas preguntas y completar porciones de código.\n",
        "\n",
        "Lectura sugerida: Sutton & Barto 2020 Caps. 3 y 4 [(link)](http://incompleteideas.net/book/RLbook2020.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe-AYvi4231C"
      },
      "source": [
        "# Paquetes a importar\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYX6RQDUc-pc"
      },
      "source": [
        "# 1. Probabilidades de transición (25p)\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqSyfwAhd319"
      },
      "source": [
        "## 1.1\n",
        "Considerar un MDP $(\\mathcal{S}, \\mathcal{A}, \\mathcal{R}, \\mathcal{P})$, donde $\\mathcal{S}$ y $\\mathcal{A}$ son los espacios de estados y acciones, discretos, con cardinalidad $N$ y $M$ respectivamente.\n",
        "\n",
        "Escribir la probabilidad de llegar al estado $s'$ luego de $T$ pasos, arrancando del estado $s$ y siguiendo la politica $\\pi(a|s)$\n",
        "\n",
        "\\begin{equation}\n",
        "P(S_T = s^\\prime | S_0 = s),\n",
        "\\end{equation}\n",
        "donde $S_T$ y $S_0$ son variables aleatorias representando el estado del sistema en tiempo $T$ y $0$. Esta es una generalizacion de las transiciones de uno y dos pasos vista en el curso (slides 21-23).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*Sugerencia*: escribir para $T=1$ y $T=2$ y usar inducción. \n",
        "\n",
        "**Tu respuesta:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvrZ9xTiEwi_"
      },
      "source": [
        "## 1.2\n",
        "\n",
        "Una forma más concisa de escribir las probabilidades de transición en varios pasos se obtiene usando matrices. Definamos la matriz $\\mathbf{P} \\in \\mathbb{R}^{N\\times N}$ cuyas entradas $(~\\mathbf{P}~)_{ij}=P\\left(S_{t+1}=s_j | S_{t}=s_i\\right)$. Diremos que esta es la *matriz de transición* del MDP inducida por la política $\\pi$.\n",
        "\n",
        "La matriz $\\mathbf{P}$ es una *matriz estocástica*, con las siguientes propiedades:\n",
        "\n",
        "* Sus entradas son no negativas\n",
        "* Sus entradas son menores o iguales a 1\n",
        "* La suma de los valores de cada fila da 1 (¿por qué?)\n",
        "\n",
        "Consideremos el vector $\\mathbf{p}_0 \\in \\mathbb{R}^N$ de entradas $(\\mathbf{p}_0)_i = P(S_o=s_i)$. Este vector representa la distribución de probabilidad del estado inicial, es decir, la probabilidad de que el MDP arranque en cada uno de los estados.\n",
        "\n",
        "Se pide:\n",
        "\n",
        "1) Obtener una expresión para $\\mathbf{p}_T \\in \\mathbb{R}^N$, vector con la distribución de probabilidad luego de $T$ pasos, en función de $\\mathbf{P}$ y $\\mathbf{p}_0$.\n",
        "\n",
        "2) Si $\\mathbf{p}_0$ contiene un $1$ en el $j$-ésimo índice y sus demás entradas son nulas, ¿qué significado tiene aplicar el resultado de la parte anterior?\n",
        "\n",
        "**Tu respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn2sGTt8fLYb"
      },
      "source": [
        "## 1.3\n",
        "\n",
        "Vamos a trabajar sobre el ejemplo del estudiante (slide 50). Los estados posibles para el estudiante son $\\texttt{happy (H)}$ o $\\texttt{sad (S)}$.\n",
        "En cada estado se puede tomar dos acciones: estudiar o tomar cerveza.\n",
        "\n",
        "Consideraremos que existen dos tipos de estudiantes (o políticas):  aquellos que siempre estudian y aquellos que siempre toman cerveza. \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1OE9yyEKlzo2QOXo4_loxcK0zM3cBYgzV\" width=500px />\n",
        "\n",
        "En esta parte se pide escribir un programa que determine la probabilidad de estar en cada estado luego de tantos pasos, siguiendo cualquiera de las dos politicas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcBXRhBY-Nkl"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqIHv0pnczIs"
      },
      "source": [
        "# Las siguientes clases implementan el MDP y una politica deterministica\n",
        "# Pueden servir como base para resolver lo que se pide mas abajo\n",
        "\n",
        "\n",
        "class StudentMDP(object):\n",
        "  \"\"\"\n",
        "  Clase básica que implementa el MDP. \n",
        "  Usar get_data() para obtener probabilidades de transición y rewards.\n",
        "  \"\"\"\n",
        "\n",
        "  ACTION_DRINK = 0\n",
        "  ACTION_STUDY = 1\n",
        "  STATE_HAPPY = 0\n",
        "  STATE_SAD = 1\n",
        "\n",
        "  def __init__(self):\n",
        "      self.data = [[[[], []], [[], []]], [[[], []], [[], []]]]\n",
        "      self.data[StudentMDP.STATE_HAPPY][StudentMDP.ACTION_DRINK][StudentMDP.STATE_HAPPY] = (1. , 10.)\n",
        "      self.data[StudentMDP.STATE_HAPPY][StudentMDP.ACTION_DRINK][StudentMDP.STATE_SAD] = (0., 0.)\n",
        "      self.data[StudentMDP.STATE_HAPPY][StudentMDP.ACTION_STUDY][StudentMDP.STATE_HAPPY] = (0.2, -10.)\n",
        "      self.data[StudentMDP.STATE_HAPPY][StudentMDP.ACTION_STUDY][StudentMDP.STATE_SAD] = (0.8, -10.)\n",
        "      self.data[StudentMDP.STATE_SAD][StudentMDP.ACTION_DRINK][StudentMDP.STATE_HAPPY] = (0.8, 40.)\n",
        "      self.data[StudentMDP.STATE_SAD][StudentMDP.ACTION_DRINK][StudentMDP.STATE_SAD] = (0.2, 40)\n",
        "      self.data[StudentMDP.STATE_SAD][StudentMDP.ACTION_STUDY][StudentMDP.STATE_HAPPY] = (0.8, 20)\n",
        "      self.data[StudentMDP.STATE_SAD][StudentMDP.ACTION_STUDY][StudentMDP.STATE_SAD] = (0.2, 20)\n",
        "\n",
        "  def get_data(self, state, action, state_prime):            \n",
        "    # Calcula la probabilidad de transición y el reward obtenido al ir de 'state'\n",
        "    # a 'state_prime' bajo la acción 'action'.\n",
        "    # Ej.: p, r = get_data(StudentMDP.STATE_HAPPY, StudentMDP.ACTION_DRINK, StudentMDP.STATE_SAD)      \n",
        "\n",
        "      if (state < 0 or state >= self.get_num_states()):\n",
        "          raise Exception(\"Invalid state\")\n",
        "      if (state_prime < 0 or state_prime >= self.get_num_states()):\n",
        "          raise Exception(\"Invalid state_prime\")\n",
        "      if (action < 0 or action >= self.get_num_actions()):\n",
        "          raise Exception(\"Invalid action\")\n",
        "      return self.data[state][action][state_prime]\n",
        "\n",
        "  def get_num_states(self):\n",
        "    # Cantidad de estados\n",
        "      return 2\n",
        "\n",
        "  def get_num_actions(self):\n",
        "    # Cantidad de acciones\n",
        "      return 2\n",
        "\n",
        "\n",
        "class DeterministicPolicy(object):\n",
        "  \"\"\"\n",
        "  Clase que describe una política determinística. Ejemplo de uso: \n",
        "  \n",
        "  policy = DeterministicPolicy(2, 2)\n",
        "  for state in [StudentMDP.STATE_HAPPY, StudentMDP.STATE_SAD]:\n",
        "    policy.set_action(state, StudentMDP.ACTION_DRINK)  \n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_states, num_actions):\n",
        "      self.num_states = num_states\n",
        "      self.num_actions = num_actions        \n",
        "      self.state_action_map = np.zeros(num_states, dtype=np.int)\n",
        "\n",
        "  def get_action(self, state):\n",
        "      if (state < 0 or state >= self.num_states):\n",
        "          raise Exception(\"Invalid state\")\n",
        "      return self.state_action_map[state]\n",
        "\n",
        "  def set_action(self, state, action):\n",
        "      if (state < 0 or state >= self.num_states):\n",
        "          raise Exception(\"Invalid state\")\n",
        "      if (action < 0 or action >= self.num_actions):\n",
        "          raise Exception(\"Invalid action\")\n",
        "      self.state_action_map[state] = action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCU4YP8h9yVE"
      },
      "source": [
        "### 1.3.1 \n",
        "Escribir una función que calcule la probabilidad de estar en $s'$ luego de $T$ pasos arrancando desde $s$, siguiendo cualquiera de estas dos politicas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8Vj0gtx90xI"
      },
      "source": [
        "# ------------\n",
        "# Tu respuesta\n",
        "# ------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM-T5YVy-YwG"
      },
      "source": [
        "### 1.3.2\n",
        "\n",
        "Usando esa función, calcular para cada estudiante las siguientes probabilidades:\n",
        "\n",
        "\n",
        "\n",
        "1.   $P(S_1 =\\texttt{happy} \\mid S_0 = \\texttt{happy})$\n",
        "2.   $P(S_1 = \\texttt{happy} \\mid S_0 = \\texttt{sad})$\n",
        "3.   $P(S_2 = \\texttt{happy} \\mid S_0 = \\texttt{happy})$\n",
        "4.   $P(S_2 = \\texttt{happy} \\mid S_0 = \\texttt{sad})$\n",
        "5.   $P(S_3 = \\texttt{happy} \\mid S_0 = \\texttt{happy})$\n",
        "6.   $P(S_3 = \\texttt{happy} \\mid S_0 = \\texttt{sad})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vcC2q7k_bcD"
      },
      "source": [
        "# ------------\n",
        "# Tu respuesta\n",
        "# ------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9kMsUzlAqdV"
      },
      "source": [
        "Calcular $P(S_2 = \\texttt{happy} \\mid S_0 = \\texttt{sad})$ usando la expresión obtenida en (1.1). Comparar con los resultados experimentales.\n",
        "\n",
        "**Tu respuesta:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajx00zPUdEc4"
      },
      "source": [
        "---\n",
        "# 2. Policy Improvement (25p)\n",
        "\n",
        "Dado un MDP $(\\mathcal{S}, \\mathcal{A}, \\mathcal{R}, \\mathcal{P})$ con factor de descuento $\\gamma$ y una política $\\pi(a|s)$, definimos la q-function como\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "q_\\pi(s, a) = \\mathbb{E}[R_t + \\gamma v_\\pi(S_{t+1}) | S_t = s, A_t = a].\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Consideremos una nueva política $\\pi^\\prime(a|s)$ tal que para todo $s\\in\\mathcal{S}$ elige $a =\\text{argmax}_{a\\in\\mathcal{A}} q_\\pi(s,a)$ una vez y luego sigue $\\pi(a|s)$. \n",
        "El Policy Improvement Theorem dice que\n",
        "\n",
        "\\begin{equation}\n",
        "v_{\\pi^\\prime} (s) \\geq v_{\\pi} (s),  \\forall s\\in\\mathcal{S}.\n",
        "\\end{equation}\n",
        "\n",
        "Vamos a demostrar esto en varios pasos. La idea de la prueba es arrancar desde un estado arbitrario $s\\in\\mathcal{S}$, dar un paso usando $\\pi^\\prime(a|s)$ y luego usar $\\pi(a|s)$. Esto debería dar un retorno esperado mayor que usar $\\pi$ siempre. Luego, consideramos dar los dos primeros pasos con $\\pi^\\prime$ y luego usar $\\pi$, y mostraremos que el retorno es mejor, y así sucesivamente. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaYpxi-lBdBk"
      },
      "source": [
        "## 2.1 \n",
        "\n",
        "Demostrar $$\\mathbb{E}_{a \\sim \\pi^\\prime}[q_\\pi(s, a)] \\geq v_\\pi(s).$$\n",
        "\n",
        "Esto quiere decir que el retorno esperado dando el primer paso con $\\pi^\\prime$ y luego seguir $\\pi$ debe ser al menos tan bueno cómo seguir $\\pi$ siempre.\n",
        "\n",
        "**Tu respuesta**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g24lWpNQB-Yi"
      },
      "source": [
        "## 2.2\n",
        "\n",
        "Definimos por conveniencia $Q_1 := \\mathbb{E}_{a \\sim \\pi^\\prime}[q_\\pi(s, a)] $.\n",
        "Consideremos ahora que la política  $\\pi^\\prime(a|s)$ aplica $a=\\text{argmax}_{a\\in\\mathcal{A}}q(s,a)$ dos veces y luego sigue $\\pi(a|s)$.\n",
        "\n",
        "Demostrar $$Q_2 := \\mathbb{E}_{\\pi^\\prime}[R_t + \\gamma R_{t+1} + \\gamma^2 v_\\pi(S_{t+2})] \\geq Q_1.$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PScn7fnPEOyr"
      },
      "source": [
        "## 2.3\n",
        "Definiendo análogamente $Q_l$ for all $l\\geq 1$ usar un razonamiento similar al de (2.2) y demostrar\n",
        "$$Q_{l+1} \\geq Q_{l}.$$\n",
        "\n",
        "**Tu respuesta**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj3M4cMSE12a"
      },
      "source": [
        "## 2.4\n",
        "Usando esto podemos concluir que $Q_\\infty \\geq v_\\pi(s)$. ¿Qué es $Q_\\infty$? \n",
        "\n",
        "**Tu respuesta**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TLls0QbE_-6"
      },
      "source": [
        "Un par de comentarios:\n",
        "\n",
        "1.   Todo el razonamiento de la prueba fue usando un estado $s\\in\\mathcal{S}$ fijo. Sin embargo, podemos construir $\\pi^\\prime$ de cualquier forma. En vez de elegir la mejor acción en un único estado, podemos hacerlo para todos los estados, ¡y la prueba sigue funcionando!\n",
        "\n",
        "2.   El enunciado del Policy Improvement Theorem es en realidad un poco más genérico:\n",
        "\n",
        " Dadas dos politicas $\\pi(a|s)$ y $\\pi^\\prime(a|s)$, si $\\mathbb{E}_{a \\sim \\pi^\\prime}[q_\\pi(s, a)] \\geq v_\\pi(s), \\forall s \\in \\mathcal{S}$, entonces\n",
        "\n",
        "\\begin{equation*}\n",
        "v_{\\pi^\\prime}(s) \\geq v_\\pi(s), \\forall s\\in\\mathcal{S}.\n",
        "\\end{equation*}\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7o_HIFmAM52"
      },
      "source": [
        "# 3. Policy Iteration (25p)\n",
        "\n",
        "Policy Iteration (Iteración de Política) es un algoritmo que aprovecha el Policy Improvement Theorem. La idea básica es comenzar con una cierta política $\\pi_0$, evaluar esa política calculando $v_{\\pi_0}(s), \\forall s\\in\\mathcal{S}$. \n",
        "Luego construye una política $\\pi_1$ que resulta de tomar la mejor acción respecto a $q_{\\pi_0}(s,a)$ para cada estado; luego se evalúa $v_{\\pi_1}(s) \\forall s\\in\\mathcal{S}$ y se repite este procedimiento para generar $\\pi_2, \\pi_3, \\ldots$ hasta que $\\pi$ converja.\n",
        "\n",
        "El Policy Improvement Theorem visto en la sección anterior garantiza que en cada paso la nueva política mejore el valor $v(\\cdot)$ en al menos un estado, salvo cuando la política original ya es optimal.\n",
        "\n",
        "Resumimos el algoritmo más abajo, tomado de [Sutton & Barto 2020, pág. 80]\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=198ZqpPx9j1dgJHozbEdsHLxz8O3_tQ_l\" width=800px/>\n",
        "\n",
        "\n",
        "## 3.1\n",
        "\n",
        "En esta parte vamos a implementar Policy Iteration. Separamos la tarea en tres funciones: la primera implementa Policy Evaluation, la segunda Policy Improvement, la tercera chequea si dos politicas son iguales.\n",
        "\n",
        "Completar cada una de las siguientes funciones.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwSQRWuj1sPr"
      },
      "source": [
        "\n",
        "# TAREA: Completar esta función.\n",
        "# Esta función implementa Policy Evaluation. Toma un StudentMDP\n",
        "# y una política para ser evaluada. Devuelve un numpy array de tamaño (num_states,)\n",
        "# que contiene la función de valor en cada estado\n",
        "\n",
        "def PolicyEvaluation(mdp, policy, gamma=.9):\n",
        "\n",
        "  N = mdp.get_num_states()\n",
        "  M = mdp.get_num_actions()\n",
        "  \n",
        "  value = np.zeros(N)\n",
        "  transition_matrix = np.zeros((N, N))\n",
        "\n",
        "  for state in range(N):\n",
        "    action = policy.get_action(state)\n",
        "    for next_state in range(N):\n",
        "      # --------------------------------\n",
        "      # Completar: calcular value[state]\n",
        "      # --------------------------------\n",
        "      pass\n",
        "  return value\n",
        "\n",
        "\n",
        "# TAREA: Completar esta función.\n",
        "# Esta función implementa Policy Improvement. Toma un MDP y una función de valor\n",
        "# (obtenida aplicando PolicyEvaluation()), calcula la q-function para cada estado\n",
        "# y devuelve una nueva política, que actúa de forma greedy respecto a q.\n",
        "\n",
        "def PolicyImprovement(mdp, value_func, gamma=.9):\n",
        "\n",
        "  N = mdp.get_num_states()\n",
        "  M = mdp.get_num_actions()\n",
        "  policy = DeterministicPolicy(N, M)\n",
        "\n",
        "  for state in range(N):\n",
        "    q_values = np.zeros(M)\n",
        "    for action in range(M):\n",
        "      # ------------------------------------\n",
        "      # Completar: calcular q_values[action]\n",
        "      # ------------------------------------    \n",
        "      pass\n",
        "\n",
        "    # --------------------------------        \n",
        "    # Completar: modificar la política\n",
        "    # --------------------------------\n",
        "      \n",
        "  return policy\n",
        "    \n",
        "\n",
        "# TAREA: Completar esta función\n",
        "# Esta función toma dos políticas old_policy y new_policy,\n",
        "# siendo ambas del tipo DeterministicPolicy().\n",
        "# devuelve True o False dependiendo de si PolicyIteration debe terminar o no.\n",
        "\n",
        "def ShouldTerminate(mdp, old_policy, new_policy):\n",
        "  # ---------\n",
        "  # Completar\n",
        "  # ---------\n",
        "  return     \n",
        "\n",
        "\n",
        "\n",
        "def PolicyIteration(mdp, initial_policy, gamma=.9):\n",
        "  policy = initial_policy\n",
        "  \n",
        "  idx = 0\n",
        "  while True:\n",
        "    value_func = PolicyEvaluation(mdp, policy, gamma)\n",
        "    new_policy = PolicyImprovement(mdp, value_func, gamma)\n",
        "\n",
        "    print(\"Iteración \" + str(idx) + \": valor = \" + str(value_func.T))\n",
        "\n",
        "    should_terminate = ShouldTerminate(mdp, policy, new_policy)\n",
        "    if (should_terminate):\n",
        "        break\n",
        "    policy = new_policy\n",
        "    idx += 1\n",
        "\n",
        "  print(\"Pronto: \" + str(value_func.T))\n",
        "  return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC3OfwpIUK_2"
      },
      "source": [
        "## 3.2\n",
        "\n",
        "Aplicar Policy Iteration en el ejemplo del ejercicio 1. Presentar tres gráficas:\n",
        "\n",
        "\n",
        "\n",
        "1.   Función de valor $v(s)$  para cada estado ($\\texttt{happy}$, $\\texttt{sad})$ en función de $k$, siendo $k$ la cantidad de veces que se ejecuta Policy Evaluation.\n",
        "2.   Tabla con la política $\\pi(a|s)$ al converger.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgVmmldqVOe4"
      },
      "source": [
        "# ------------\n",
        "# Tu respuesta\n",
        "# ------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDil4ziFWVu9"
      },
      "source": [
        "Desde un punto de vista computacional, ¿qué desventajas le encontrás a este algoritmo?\n",
        "\n",
        "**Tu respuesta**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Im44CXKgqQI"
      },
      "source": [
        "# 4. Value iteration (25p)\n",
        "\n",
        "\n",
        "Una de las desventajas de Policy Iteration es que en cada paso evalúa la política actual. Esto a la larga puede ser computacionalmente exigente, además de que no vale la pena evaluar perfectamente una política que en el siguiente paso se va a modificar.\n",
        "\n",
        "Una alternativa es actualizar alternadamente la función de valor y la política. Este algoritmo se llama Value Iteration (Iteración de Valor) [Sutton & Barto 2020, pág. 83]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=19l-A5QLwpS1WArveVtPCc-4Vi0YcP8rZ\" width=800px/>\n",
        "\n",
        "## 4.1\n",
        "Implementar el algoritmo de Value Iteration\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD8Rxk4shRFY"
      },
      "source": [
        " def ValueIteration(mdp, initial_policy, gamma=.9):\n",
        "  # ---------\n",
        "  # Completar\n",
        "  # ---------\n",
        "  return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsR1Zhge272Z"
      },
      "source": [
        "## 4.2\n",
        "\n",
        " Aplicar el algoritmo para el MDP del Ejercicio 1. Mostrar:\n",
        "\n",
        "\n",
        "\n",
        "1.   Evolución de la función de valor para cada estado, en función del número de iteraciones del loop de más afuera \n",
        "2.   Tabla con la política $\\pi(a|s)$ al converger."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KrgJxKH3Ia6"
      },
      "source": [
        "# ------------\n",
        "# Tu respuesta\n",
        "# ------------"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}